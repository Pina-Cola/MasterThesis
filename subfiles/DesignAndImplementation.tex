\documentclass[../MasterThesis.tex]{subfiles}
\graphicspath{ {./assets/images/} }


%----------------------------------------------------------------------------
%----------------------------------------------------------------------------

\begin{document}
	
	
	
%
%
%
%
%=======================================================================================================
%
%
%
%
%=======================================================================================================
% CHAPTER: DESIGN AND IMPLEMENTATION
%=======================================================================================================
\newpage
\section{Design and Implementation} \label{section:designandimplementation}







%-------------------------------------------------------------------------------------------------------
\subsection{Architecture Design} \label{subsection:architecturedesign}
% Describe the overall architecture of the system.

%Provide an overview of the system architecture and design. This could include high-level diagrams, such as system flowcharts or architecture diagrams, to illustrate the structure of the software.
%Introduce the existing codebase and its components. Briefly describe the major modules or sections of the code.
%Explain the design decisions made during the development process. Discuss the rationale behind the chosen architecture, programming languages, and frameworks.
%Present any relevant design patterns or paradigms applied in the codebase.
%Provide a detailed description of how the existing code works. This involves explaining the functionality of key modules or sections, the flow of data, and how different components interact.
%If applicable, discuss any algorithms or data structures implemented in the codebase.
%Highlight any challenges or considerations encountered during the implementation phase and how they were addressed.
%Include code snippets, illustrations, or diagrams to enhance the understanding of your codebase.

TODO: Short overview/introduction of architecture and explain structure of this section?





The connection from the Python backend to the Melt framework can use Melt's capabilities for video editing, processing, transcoding, rendering or playback. The system includes real-time communication via WebRTC, where Melt is involved in handling and manipulating video content.














%-------------------------------------------------------------------------------------------------------
\subsection{Accurate Video} \label{subsection:accuratevideo}
% Describe the frontend


The frontend uses Node.js as its runtime environment. Node.js enables the execution of JavaScript code outside of a web browser.~\cite{nodejs, RM_Frontend, ap3_docs}

To manage project dependencies and packages, either Yarn or npm (Node Package Manager) is used. These package managers allow developers to install, update, and manage libraries and tools used in the project.~\cite{RM_Frontend, npmyarn}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{IM_FE.png}
	\caption{Accurate Video in the system architecture}
\end{figure}













%-------------------------------------------------------------------------------------------------------
\subsection{JIT-WebRTC} \label{subsection:jit-webrtc}
% Describe the backend

The following description of the system is based on the code base and the information from the \texttt{README} file~\cite{RM_Backend}.

JIT-WebRTC is a live transcoder with a WebRTC output. As described in Section~\ref{subsection:technicalbackground}, transcoding is the conversion of one digital data format into another.~\cite{transcoding}

%# jit-webrtc
%
%Live Transcoder with WebRTC output
%

The backend consists of three components that will be described in the following:
\begin{itemize}
	\item Melt
	\item \texttt{main.py}
	\item Session service
\end{itemize}

The system's frontend was described in Section \ref{subsection:accuratevideo}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\textwidth]{IM3.png}
	\caption[System architecture]{Architecture of the system that consits out of four parts.}
\end{figure}

%## Design 
%
%The system consists of four "moving parts": three on the backend (`melt`, `main.py` and `session-service`)
%and one on the frontend (website with a JavaScript/WebRTC client).
%
%```
%                  +---------+      +---------+      +---------+
%                  |         | init |         | init |         |
%                  |         |<-----| session |<-----|         |
%+------+   AVI    |         |      |         |      |         |
%|      |--------->|         |      +---------+      |         |
%|      | metadata |         |         video         |         |
%| melt |--------->| main.py |---------------------->| browser |
%|      | control  |         |         audio         |         |
%|      |<---------|         |---------------------->|         |
%+------+          |         |        metadata       |         |
%                  |         |---------------------->|         |
%                  |         |        control        |         |
%                  |         |<----------------------|         |
%                  +---------+                       +---------+
%```
%





%---------------------------------
\subsubsection{Data flow}

The user initiates a session through the browser, which then initiates the \texttt{main.py} script.
Control commands are then sent from the browser to \texttt{main.py} with a WebRTC data channel.

From the \texttt{main.py}, the control commands get sent to Melt via \texttt{stdout} (standard output) and \texttt{stdin} (standard input). These are the default input and output channels, that allow the Python script communication with other components.
Data written to \texttt{stdout} by the Python code can be captured or redirected and data from an external source can be read from \texttt{stdin} by the Python script. \cite{python}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{IM_control.png}
	\caption{Control commands in the system architecture}
	
\end{figure}


Melt sends an AVI stream with the rendered video and audio to the \texttt{main.py} via a named pipe and status messages and metadata via another named pipe.
AVI is a file format that can contain audio and video information to allow synchronized playback of audio and video components.~\cite{avi} AVI was further described in Section~\ref{subsection:technicalbackground} and named pipes in Section~\ref{subsection:technicalbackground}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{IM_avi.png}
	\caption{AVI stream in the system architecture}
\end{figure}


Then the audio and video from the incoming AVI stream is extracted by the \texttt{main.py} script to then encode it for sending it to the browser with WebRTC. The WebRTC connection serves as a direct communication channel between the browser and the backend, enabling real-time data exchange, involving audio, video, or other data streams. 

TODO: Is is encoded into h-264 here?

%It has the ability to extract specific channels from the rendered audio, to %be rendered to an stereo stream for WebRTC.
%It does not appear to be possible to encode surround sound for WebRTC, despite WebRTC using the Opus codec which is surround capable.
%This may be a limitation in `aiortc`, in WebRTC or in the browser, or all three.
%

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{IM_wrtc.png}
	\caption{WebRTC connection in the system architecture}
\end{figure}

Then the JavaScript client of the browser receives the video and audio and plays it. In addition to this, it
receives metadata from the \texttt{main.py} script and sends commands and ping responses over the control channel.


\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{IM_wrtc_control.png}
	\caption[WebRTC and control in the system architecture]{WebRTC connection and control commands in the system architecture}
\end{figure}

Melt and the \texttt{main.py} script are started once for each JIT session with a parameter that presents the media from the start on.

%`main.py` and `melt` is started once for each JIT session, and media is presented to it on startup. 

\subsubsection{Session}

\texttt{Session} is a java-based service, designed to support machine-to-machine interaction between the browser and the backend (\texttt{main.py} and Melt). Web services allow different applications, that often run on different platforms and are written in different programming languages, to communicate with each other.~\cite{webservice}

The \texttt{Session} service exposes a REST API (described in Section~\ref{subsection:technicalbackground}) for initiating a JIT session with specified media details. 

% Additionally, it leverages AWS, specifically utilizing the Auto Scaling Group (ASG) feature to manage a cluster of instances dynamically, ensuring scalability and efficient distribution of JIT sessions across the instances in the cluster.

% AWS
%2. **AWS (Amazon Web Services):**
%- **Definition:** AWS is a cloud computing platform provided by Amazon. It offers a wide range of services, including computing power, storage, databases, machine learning, and more, allowing businesses to scale and grow without the need for significant upfront investments in physical infrastructure.

% ASG cluster
%3. **ASG Cluster (Auto Scaling Group):**
%- **Definition:** An Auto Scaling Group is an AWS service that automatically adjusts the number of compute instances (e.g., virtual machines) in response to changes in demand or other specified conditions. It helps ensure that the desired number of instances are available to handle varying workloads.



%`session` is a Java-based service that presents a REST api that can be used to start a new JIT session given a description of the media to use. It also has support for managing a full ASG cluster on AWS, and distribute new JIT sessions on the instances in 
%the ASG.
%
%For more information on each component see:
%* [`melt`](https://github.com/sirf/mlt.git)
%* [`main.py`](jit/README.md)
%* [`session`](session/README.md)
%

For development and quick testing without the session service, JIT can be started in a Docker container. A Docker container is in isolated environment in which an application can be run and Docker is the platform that enables the usage of those containers.~\cite{docker}


\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\textwidth]{IM2.png}
	\caption{System architecture when run in a docker container.}
\end{figure}


%## Development
%
%For development and quick testing without the session service, on can either start JIT as a docker, or run JIT directly on your computer. The latter has some problems if you're on an Apple ARM.
%
%Run the docker:
%
%`docker/main/main.sh --threads 16 --port 8080 $VIDEOFILE`
%
%Where `$VIDEOFILE` is either a local path or a URL. `threads` and `port` are both optional, with current default values set to 72 threads and port 8080.
%
%To run JIT directly, see [`jit/README.md`](jit/README.md).
%
%### Open Docker IPs
%
%Your Docker container will get an IP in the style of 172.17.0.\*. Using e.g. Docker CE on Linux this IP will be reachable from outside the container. However, on Docker Desktop for Mac, this IP will not be opened. This will cause problems. There is a service that can fix this. You **install and activate it once** by doing this:
%
%```
%# Install via Homebrew
%brew install chipmk/tap/docker-mac-net-connect
%
%# Run the service and register it to launch at boot
%sudo brew services start chipmk/tap/docker-mac-net-connect
%```
%
%
%## Deployment
%
%Currently the only way supported for deployment is by using EC2 instances on AWS. An AMI is created by CI on release, containing a 
%local instance of the `session-service`, together with `main.py`, `melt` and anything else needed to start JIT processes on the EC2 
%instance. For more information about what the AMI contains, and how configuration can be fed to it see [`ami/README.md`](ami/README.md).
%
%### Terraform
%
%Terraform modules are supplied that simplify setup of either single EC2 instances or a full ASG cluster of instances, including 
%`session-service` running as an orchestrator on ECS. See [`terraform/README.md`](terraform/README.md) for more information on 
%available modules, their parameters and outputs.
%





\newpage
%-------------------------------------------------------------------------------------------------------
\subsection{Melt Filter Comparison} \label{subsection:meltfilter}



The goal for this thesis project is to adjust the RGB values of a video with sliders. For this purpose, various Melt filters that seemed suitable were examined and compared to identify the most suitable ones for implementing this functionality. RGB colour representation was described in Section~\ref{subsection:RGB} and the Melt framework was described in Section~\ref{subsection:melt}.

In the following, a selection of filters, that influence the RGB representation of the video are listed. This includes the filter name, description and possible parameters from the Melt website.~\cite{melt_filters} Only the relevant parameters will be listed.
Those filters were then executed locally, to compare the visual results of applying them.


\begin{table}[H]
	\footnotesize
	\begin{tabular}{lp{4.4cm}p{4.5cm}}
		\toprule
		Name & Description & Parameters \\
		\midrule
		\texttt{avfilter.colorbalance} & Adjust the colour balance & 
		\tiny{
		\texttt{av.rs}: set red shadows \newline 
		\texttt{av.gs}: set green shadows \newline 
		\texttt{av.bs}: set blue shadows \newline 
		\texttt{av.rm}: set red mid tones \newline 
		\texttt{av.gm}: set green mid tones \newline 
		\texttt{av.bm}: set blue mid tones \newline 
		\texttt{av.rh}: set red highlights \newline 
		\texttt{av.gh}: set green highlights \newline 
		\texttt{av.bh}: set blue highlights}
		\\
		\texttt{avfilter.colorchannelmixer} & Adjust colours by mixing colour channels & 
		\tiny{
		\texttt{av.rr}: set red gain for red channel \newline 
		\texttt{av.rg}: set green gain for red channel \newline 
		\texttt{av.rb}: set blue gain for red channel \newline 
		\texttt{av.ra}: set alpha gain for red channel \newline 
		\texttt{av.gr}: set red gain for green channel \newline 
		\texttt{av.gg}: set green gain for green channel \newline 
		\texttt{av.gb}: set blue gain for green channel \newline 
		\texttt{av.ga}: set alpha gain for green channel \newline 
		\texttt{av.br}: set red gain for blue channel \newline 
		\texttt{av.bg}: set green gain for blue channel \newline 
		\texttt{av.bb}: set blue gain for blue channel \newline 
		\texttt{av.ba}: set red gain for alpha channel \newline 
		\texttt{av.ar}: set red gain for alpha channel \newline 
		\texttt{av.ag}: set green gain for alpha channel \newline 
		\texttt{av.ab}: set blue gain for alpha channel \newline 
		\texttt{av.aa}: set alpha gain for alpha channel}
		\\
		\texttt{frei0r.coloradj\_RGB} & Simple colour adjustment & 
		\tiny{
		\texttt{R}: amount of red \newline 
		\texttt{G}: amount of green \newline 
		\texttt{B}: amount of blue}
		\\
		\bottomrule
	\end{tabular}
	\caption[List of Melt filters that influence the colour of a video.]{List of different Melt filters that influence the colour of a video with their name, description and parameters.}
\end{table}



%Execution of \texttt{melt https://s3.eu-central-1.amazonaws.com/accurate-player\--demo-assets/timecode/sintel-2048-timecode-stereo.mp4 \\ -filter avfilter.colorbalance av.rs=1 av.gm=1 av.bh=1}:


%\begin{minipage}{0.5\textwidth}
%	\includegraphics[width=0.9\textwidth]{colourdefault.png}
%	Original colours
%\end{minipage}\begin{minipage}{0.5\textwidth}
%	\includegraphics[width=0.9\textwidth]{colourhigh.png}
%	Colours with \texttt{av.rs=1} \texttt{av.gm=1} \texttt{av.bh=1}
%\end{minipage}

%Adding filter to \texttt{local\_melt.py} to execute it in the Accurate Player using JIT:
%`
%\begin{center}
%	\includegraphics[height=0.13\textwidth]{code.png}
%	\includegraphics[height=0.13\textwidth]{codecleaner.png}
%\end{center}
%
%\begin{center}
%	\includegraphics[width=0.8\textwidth]{ap_red.png}
%\end{center}


To compare the listed filters and their parameters, different values to change the red tones of the video will be tested and compared, assuming that the change of green and blue works analogous.

In addition to this, it needs to be tested, if multiple parameters of the same filter can be applied simultaneously. For evaluating, the filters and their different parameters for green and red are applied to the same frame of the test video file. This should result in a yellow output. 

Two frames with different colour schemes and content are used for those tests. One shows a snow landscape and the \textit{Blender Institute Production}-text and the other one shows an old man. The original frames $343$ and $3377$ can be seen in Figure~\ref{figure:nofilter}.

\begin{figure}[H]
	\begin{center}
		\cutpic{0.3cm}{0.45\textwidth}{nofilter_snow.png}
		\cutpic{0.3cm}{0.45\textwidth}{nofilter_man.png}
		\caption[Frames $343$ and $3377$ from the test video file without a filter.]{Frames $343$ on the left and $3377$ on the right from the test video file without a filter.}
		\label{figure:nofilter}
	\end{center}
\end{figure}

The filters were applied with the following CLI command:
\begin{lstlisting}[language=bash, numbers=none]
	melt -filter <filter_name> <filter_parameter> https://s3.eu-central-1.amazonaws.com/accurate-player-demo-assets/timecode/sintel-2048-timecode-stereo.mp4 -consumer xgl
\end{lstlisting}















%--------------------------------------------------------------------------------------
\subsubsection*{Filter \texttt{avfilter.colorbalance}}

The parameters of this filter take input values as float between $-1$ and $1$. It has individual parameters to set the shadows, mid tones and highlights per colour. This can be seen in Figure~\ref{figure:rs1rm1rh1}.

\begin{figure}[H]
	\begin{center}
		\cutpic{0.32cm}{0.3\textwidth}{rs_snow.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{rm_snow.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{rh_snow.png}
		% \small{
		%\texttt{av.rs=1} \hspace*{0.22\textwidth} \texttt{av.rm=1} \hspace*{0.23\textwidth} \texttt{av.rh=1}}
		\cutpic{0.32cm}{0.3\textwidth}{rs_man.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{rm_man.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{rh_man.png}
		\small{
		\texttt{av.rs=1} \hspace*{0.22\textwidth} \texttt{av.rm=1} \hspace*{0.23\textwidth} \texttt{av.rh=1}}
		\caption[Different parameters from the filter \texttt{avfilter.colorbalance} applied.]{Different parameters from the filter \texttt{avfilter.colorbalance} applied: Left with \texttt{av.rs=1} for red shadows, middle with \texttt{av.rm=1} for red mid tones and right with \texttt{av.rh=1} for red highlights.}
			\label{figure:rs1rm1rh1}
	\end{center}
\end{figure}

To get an even result of the colour on the frame, which would be applicable for the colour change with one slider. In Figure~\ref{figure:rsrmrh1}, the shadows, mid tones and highlights for the red value are set to $1$. 

\begin{figure}[H]
	\begin{center}
		\cutpic{0.3cm}{0.45\textwidth}{rsrmrh_snow.png}
		\cutpic{0.3cm}{0.45\textwidth}{rsrmrh_man.png}
		\label{figure:rsrmrh1}
		\caption[Parameters set to $1$ for red using the \texttt{avfilter.colorbalance} filter.]{All parameters for red with the value $1$  from the filter \texttt{avfilter.colorbalance} applied: \texttt{av.rs=1}, \texttt{av.rm=1} and \texttt{av.rh=1}.}
	\end{center}
\end{figure}

To evaluate if and how the overlay of different filters works, the values for green and red shadows, mid tones and highlights are set to $1$ and applied together. The parameters that are applied are \texttt{av.rs=1}, \texttt{av.rm=1}, \texttt{av.rh=1}, \texttt{av.gs=1}, \texttt{av.gm=1} and \texttt{av.gh=1}.
This results in yellow toned pictures.


\begin{figure}[H]
	\begin{center}
		\cutpic{0.3cm}{0.45\textwidth}{cb_yellow.png}
		\cutpic{0.3cm}{0.45\textwidth}{cb_yellow_man.png}
		\label{figure:cb_yellow}
		\caption[Red and green parameters set to $1$ with \texttt{avfilter.colorbalance}.]{All parameters for red and green with the value $1$  from the filter \texttt{avfilter.colorbalance} applied: \texttt{av.rs=1}, \texttt{av.rm=1}, \texttt{av.rh=1}, \texttt{av.gs=1}, \texttt{av.gm=1} and \texttt{av.gh=1}.}
	\end{center}
\end{figure}













%--------------------------------------------------------------------------------------

\subsubsection*{Filter \texttt{avfilter.colorchannelmixer}}

The parameters of this filter take input values as float between $-2$ and $2$. It has individual parameters to set the red, green, blue or alpha gain for the red, green, blue or alpha channel. The application of those parameters can be seen in Figure~\ref{figure:rrraar}.

\begin{figure}[H]
	\begin{center}
		\cutpic{0.32cm}{0.3\textwidth}{rr_snow.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{ra_snow.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{ar_snow.png}
		% \small{
			%\texttt{av.rs=1} \hspace*{0.22\textwidth} \texttt{av.rm=1} \hspace*{0.23\textwidth} \texttt{av.rh=1}}
		\cutpic{0.32cm}{0.3\textwidth}{rr_man.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{ra_man.png}
		%\hspace*{0.01\textwidth}
		\cutpic{0.32cm}{0.3\textwidth}{ar_man.png}
		\small{
			\texttt{av.rr=2} \hspace*{0.22\textwidth} \texttt{av.ra=2} \hspace*{0.23\textwidth} \texttt{av.ar=2}}
		\caption[Different parameters from \texttt{avfilter.colorchannelmixer} applied.]{Different parameters from the filter \texttt{avfilter.colorchannelmixer} applied: Left with \texttt{av.rr=2} for red gain on the red channel, middle with \texttt{av.ra=2} for alpha gain on the red channel and right with \texttt{av.ar=2} for red gain on the alpha channel.}
		\label{figure:rrraar}
	\end{center}
\end{figure}


In Figure~\ref{figure:rrbrgr}, the red gain on the red, blue and green channel is applied. The parameter setting are \texttt{av.rr=2}, \texttt{av.gr=2} and \texttt{av.br=2}.


\begin{figure}[H]
	\begin{center}
		\cutpic{0.3cm}{0.45\textwidth}{rrbrgr_snow.png}
		\cutpic{0.3cm}{0.45\textwidth}{rrbrgr_man.png}
		\label{figure:rrbrgr}
		\caption[Red gain set to $2$ with \texttt{avfilter.colorchannelmixer}.]{Red gain with the value $2$ for the red, green and blue channel from the filter \texttt{avfilter.colorchannelmixer} applied: \texttt{av.rr=2}, \texttt{av.gr=2} and \texttt{av.br=2}.}
	\end{center}
\end{figure}


To evaluate if and how the overlay of different filters works, the values for the green and red gain on the green and red channel are set to $2$ and applied together. The parameters that are applied are \texttt{av.rr=2} and \texttt{av.gg=2}. This results in yellow toned pictures.


\begin{figure}[H]
	\begin{center}
		\cutpic{0.3cm}{0.45\textwidth}{rrgg_snow.png}
		\cutpic{0.3cm}{0.45\textwidth}{rrgg_man.png}
		\label{figure:rrgg}
		\caption[Red and green gain set to $2$ with \texttt{avfilter.colorchannelmixer}.]{Red and green gain on the red and green channel with the value $2$ from the filter \texttt{avfilter.colorchannelmixer} applied: \texttt{av.rr=2} and \texttt{av.gg=2}.}
	\end{center}
\end{figure}







%--------------------------------------------------------------------------------------
\subsubsection*{Filter \texttt{frei0r.coloradj\_RGB}}

The parameters of this filter take input values as float between $0$ and $1$. It has individual parameters to set the red, green, blue value. The application of the red parameters can be seen in Figure~\ref{figure:r}.


\begin{figure}[H]
	\begin{center}
		\cutpic{0.3cm}{0.45\textwidth}{r_snow.png}
		\cutpic{0.3cm}{0.45\textwidth}{r_man.png}
		\label{figure:r}
		\caption[Red parameter set to $1$ with \texttt{frei0r.coloradj\_RGB}.]{Red parameter with the value $1$ from the filter \texttt{frei0r.coloradj\_RGB} applied: \texttt{R=1}.}
	\end{center}
\end{figure}



To evaluate if and how the overlay of different filters works, the values for the green and red parameter are set to $1$ and applied together. The parameters that are applied are \texttt{R=1} and \texttt{G=1}. This results in yellow toned pictures, which can be seen in Figure~\ref{figure:rg}.


\begin{figure}[H]
	\begin{center}
		\cutpic{0.3cm}{0.45\textwidth}{rg_snow.png}
		\cutpic{0.3cm}{0.45\textwidth}{rg_man.png}
		\label{figure:rg}
		\caption[Red and green parameter set to $1$ with \texttt{frei0r.coloradj\_RGB}.]{Red and green parameter with the value $1$ from the filter \texttt{frei0r.coloradj\_RGB} applied: \texttt{R=1} and \texttt{G=1}.}
	\end{center}
\end{figure}









\newpage
%-------------------------------------------------------------------------------------------------------
\subsection{Technology} \label{subsection:technology}
% Discuss the technologies and tools chosen for the implementation.








	
	
	
\end{document}